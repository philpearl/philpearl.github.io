<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Memoryleak on EnTitled</title>
    <link>https://philpearl.github.io/categories/memoryleak/</link>
    <description>Recent content in Memoryleak on EnTitled</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <lastBuildDate>Thu, 15 Nov 2018 19:57:02 +0000</lastBuildDate>
    
	<atom:link href="https://philpearl.github.io/categories/memoryleak/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Further Dangers of Large Heaps in Go</title>
      <link>https://philpearl.github.io/post/danger_large_heap/</link>
      <pubDate>Thu, 15 Nov 2018 19:57:02 +0000</pubDate>
      
      <guid>https://philpearl.github.io/post/danger_large_heap/</guid>
      <description>Over my years working with Go I stumble across more and more reasons why having a large amount of data in memory in Go is a headache.
The latest issue is a problem with the bulk feature extraction process we use at Ravelin (yes, we’re hiring! So if you like Go and you’re anywhere near London drop us a line). For our larger clients we’ve found this process just uses more and more memory, so we keep having to run it on more and more expensive boxes.</description>
    </item>
    
  </channel>
</rss>